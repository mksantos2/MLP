{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "rkqvaVl_NICy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Objetivos deste trabalho\n",
        "- Familiarizar-se com a biblioteca PyTorch\n",
        "- Definir arquiteturas MLP simples em PyTorch\n",
        "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
        "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
      ]
    },
    {
      "metadata": {
        "id": "rNeRYBlnNIC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnJ3PA4ENIDA",
        "colab_type": "code",
        "outputId": "1eb2705d-09b4-41e3-942f-ae5e57a308b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Carregar os datasets\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_valid, dataset_train = torch.utils.data.random_split(dataset_train, [5000, 45000])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x97-R2fxNIDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=256)\n",
        "valid_loader = DataLoader(dataset=dataset_valid, shuffle=False, batch_size=256)\n",
        "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2kEsoD-NIDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura MLP\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 80)\n",
        "        self.fc2 = nn.Linear(80, 70)\n",
        "        self.fc3= nn.Linear(70, 50)\n",
        "        self.fc4= nn.Linear(50, 10)  \n",
        "        self.activation_function = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = self.activation_function(self.fc1(x))\n",
        "        x = self.activation_function(self.fc2(x))\n",
        "        x = self.activation_function(self.fc3(x))\n",
        "        x = self.activation_function(self.fc4(x))\n",
        "        #x = self.activation_function(self.fc5(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NC8JZneqNIDO",
        "colab_type": "code",
        "outputId": "30a2333d-f5d7-4a8c-e4ab-52d4d820ce03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "model = MLP()\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=1024, out_features=80, bias=True)\n",
            "  (fc2): Linear(in_features=80, out_features=70, bias=True)\n",
            "  (fc3): Linear(in_features=70, out_features=50, bias=True)\n",
            "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (activation_function): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CxXXF-iNNIDS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir otimizador e loss\n",
        "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#loss_fn = torch.nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhpgpfaoNIDV",
        "colab_type": "code",
        "outputId": "8bb4b441-e8c3-485e-8684-bbb525e194ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1835
        }
      },
      "cell_type": "code",
      "source": [
        "# Realizar o treinamento aqui\n",
        "\n",
        "epochs = 100\n",
        "one_hot = torch.eye(10)\n",
        "media_losses = []\n",
        "media_ac = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs): \n",
        "  \n",
        "  model.train()\n",
        "  losses = []\n",
        "  correct = 0\n",
        "  #total = 0\n",
        "  for img, category in train_loader:\n",
        "        \n",
        "    #zera o gradiente no começo do batch\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    ys = model(img)\n",
        "    \n",
        "    #one_hot_category = one_hot[category] # apenas para quando for usar MSELoss\n",
        "\n",
        "    loss = loss_fn(ys, category #one_hot_category)\n",
        "    \n",
        "    \n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    #_, predicted = torch.max(ys.data, 1) # pega o segundo argumento que retorna de torch.max\n",
        "    # correct += (predicted == category).sum().item()\n",
        "    #total += category.size(0)\n",
        "    \n",
        "    \n",
        "    #print(one_hot_category, category)\n",
        "    \n",
        "  \n",
        " #ac = correct/len(train_loader)\n",
        "  #media_ac.append(ac)\n",
        "   #print(\"epoch : {}, train loss : {:.4f}, acc_train : {:.2f}%\".format(epoch, np.mean(losses), ac))\n",
        "  \n",
        "  print(\"Época: \", epoch+1,\" \", \"Loss: \", np.mean(losses)) # importante mencionar que está sendo printada a média dos losses\n",
        "    \n",
        "  media_losses.append(np.mean(losses)) # losses para o plot mais embaixo\n",
        "  \n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época:  1   Loss:  2.2172561572356657\n",
            "Época:  2   Loss:  2.125988792289387\n",
            "Época:  3   Loss:  2.0804849314418705\n",
            "Época:  4   Loss:  2.0484395108439704\n",
            "Época:  5   Loss:  1.983842713589018\n",
            "Época:  6   Loss:  1.945270985364914\n",
            "Época:  7   Loss:  1.919635377146981\n",
            "Época:  8   Loss:  1.9006481590596112\n",
            "Época:  9   Loss:  1.8805067424069752\n",
            "Época:  10   Loss:  1.865356064655564\n",
            "Época:  11   Loss:  1.851443275809288\n",
            "Época:  12   Loss:  1.836578407748179\n",
            "Época:  13   Loss:  1.8267145793546329\n",
            "Época:  14   Loss:  1.8149135939099572\n",
            "Época:  15   Loss:  1.808216628703204\n",
            "Época:  16   Loss:  1.7979534769600087\n",
            "Época:  17   Loss:  1.7888574342836032\n",
            "Época:  18   Loss:  1.7785804183645681\n",
            "Época:  19   Loss:  1.7677046047015623\n",
            "Época:  20   Loss:  1.768273357640613\n",
            "Época:  21   Loss:  1.7603198052807287\n",
            "Época:  22   Loss:  1.7499599301002242\n",
            "Época:  23   Loss:  1.7480204288255086\n",
            "Época:  24   Loss:  1.7385176901112904\n",
            "Época:  25   Loss:  1.7257301326502452\n",
            "Época:  26   Loss:  1.7253597541288896\n",
            "Época:  27   Loss:  1.7254106422716922\n",
            "Época:  28   Loss:  1.7158384465358474\n",
            "Época:  29   Loss:  1.7141663107005032\n",
            "Época:  30   Loss:  1.7053628787398338\n",
            "Época:  31   Loss:  1.698365767571059\n",
            "Época:  32   Loss:  1.6983908983794125\n",
            "Época:  33   Loss:  1.693583669987592\n",
            "Época:  34   Loss:  1.6852836134758862\n",
            "Época:  35   Loss:  1.6797649745236745\n",
            "Época:  36   Loss:  1.6762300404635342\n",
            "Época:  37   Loss:  1.6748658391562374\n",
            "Época:  38   Loss:  1.67446876114065\n",
            "Época:  39   Loss:  1.6666891954161904\n",
            "Época:  40   Loss:  1.6635777300054377\n",
            "Época:  41   Loss:  1.653798637742346\n",
            "Época:  42   Loss:  1.6547390148043633\n",
            "Época:  43   Loss:  1.6481910097328099\n",
            "Época:  44   Loss:  1.644680174236948\n",
            "Época:  45   Loss:  1.644575322893533\n",
            "Época:  46   Loss:  1.6378687979145483\n",
            "Época:  47   Loss:  1.6349376311356372\n",
            "Época:  48   Loss:  1.6271566958589987\n",
            "Época:  49   Loss:  1.6285395662892947\n",
            "Época:  50   Loss:  1.6292396221648564\n",
            "Época:  51   Loss:  1.619199387729168\n",
            "Época:  52   Loss:  1.6161358011039821\n",
            "Época:  53   Loss:  1.6134036501700229\n",
            "Época:  54   Loss:  1.6113258132880384\n",
            "Época:  55   Loss:  1.6063222648067907\n",
            "Época:  56   Loss:  1.6036141704429279\n",
            "Época:  57   Loss:  1.5984822870655493\n",
            "Época:  58   Loss:  1.5969051027839833\n",
            "Época:  59   Loss:  1.5923792923038655\n",
            "Época:  60   Loss:  1.5904853743585674\n",
            "Época:  61   Loss:  1.5874763009223072\n",
            "Época:  62   Loss:  1.5862492668357762\n",
            "Época:  63   Loss:  1.5814307088201696\n",
            "Época:  64   Loss:  1.577004524794492\n",
            "Época:  65   Loss:  1.578442757102576\n",
            "Época:  66   Loss:  1.5753106772899628\n",
            "Época:  67   Loss:  1.5718786987391384\n",
            "Época:  68   Loss:  1.5672820183363827\n",
            "Época:  69   Loss:  1.5668059810996056\n",
            "Época:  70   Loss:  1.5660914975133808\n",
            "Época:  71   Loss:  1.560425771231001\n",
            "Época:  72   Loss:  1.5543379533019932\n",
            "Época:  73   Loss:  1.5540897270495242\n",
            "Época:  74   Loss:  1.5594341578808697\n",
            "Época:  75   Loss:  1.5532112446698276\n",
            "Época:  76   Loss:  1.5477433692325244\n",
            "Época:  77   Loss:  1.5436144044453448\n",
            "Época:  78   Loss:  1.5455520938743244\n",
            "Época:  79   Loss:  1.5421427515420048\n",
            "Época:  80   Loss:  1.5368249287659472\n",
            "Época:  81   Loss:  1.537282641638409\n",
            "Época:  82   Loss:  1.5319562832062894\n",
            "Época:  83   Loss:  1.5313491577451879\n",
            "Época:  84   Loss:  1.5329430679028684\n",
            "Época:  85   Loss:  1.532365309921178\n",
            "Época:  86   Loss:  1.529266361485828\n",
            "Época:  87   Loss:  1.5291698127985\n",
            "Época:  88   Loss:  1.5212261507456952\n",
            "Época:  89   Loss:  1.5197580151937224\n",
            "Época:  90   Loss:  1.517680480398915\n",
            "Época:  91   Loss:  1.5179369063539938\n",
            "Época:  92   Loss:  1.512625046751716\n",
            "Época:  93   Loss:  1.5096126280047677\n",
            "Época:  94   Loss:  1.5127123560417781\n",
            "Época:  95   Loss:  1.511704757809639\n",
            "Época:  96   Loss:  1.5091380517591129\n",
            "Época:  97   Loss:  1.5033784048123793\n",
            "Época:  98   Loss:  1.5065144686536356\n",
            "Época:  99   Loss:  1.4982121546160092\n",
            "Época:  100   Loss:  1.4947791187600656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CQ9ptdfY2dM4",
        "colab_type": "code",
        "outputId": "539c38c5-e273-4952-e8ad-9ae0c95af66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "#graphs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"Epoca\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gráfico Loss\")\n",
        "plt.plot(media_losses)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1yeqVuZJNIDY",
        "colab_type": "code",
        "outputId": "5f73b7c8-7036-44ba-f898-e764295ed519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo aqui (no conjunto de avaliação)\n",
        "correct = 0\n",
        "valid_losses = []\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, category in valid_loader:\n",
        "                y = model(images)\n",
        "\n",
        "                #one_hot_category = one_hot[category]\n",
        "                loss = loss_fn(y, category)\n",
        "\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                _, predicted = torch.max(y.data, 1)\n",
        "                correct += (predicted == category).sum().item()\n",
        "                total += category.size(0)\n",
        "      \n",
        "#ac = correct/len(valid_loader)\n",
        "ac = correct/total\n",
        "print(\"Loss_valid: \", np.mean(valid_losses), \" \", \"Accuracy_valid: \", ac)\n",
        "\n",
        "correct = 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_valid:  1.8225641965866088   Accuracy_valid:  0.3982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PaANB3Nu1RcB",
        "colab_type": "code",
        "outputId": "f5c26729-07ea-4e36-a98c-d655bec75a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Testar o modelo aqui \n",
        "\n",
        "\n",
        "test_losses = []\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, category in test_loader:\n",
        "                yt = model(images)\n",
        "\n",
        "                #one_hot_category = one_hot[category]\n",
        "                loss = loss_fn(yt, category)\n",
        "\n",
        "\n",
        "                test_losses.append(loss.item())\n",
        "\n",
        "                _, predicted = torch.max(yt.data, 1)\n",
        "                correct += (predicted == category).sum().item()\n",
        "                total += category.size(0)\n",
        "\n",
        "\n",
        "\n",
        "ac = correct/total \n",
        "\n",
        "#ac = correct/len(test_loader)\n",
        "print(\"Loss_test: \", np.mean(test_losses), \" \", \"Accuracy_test: \", ac)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_test:  1.8141047567129136   Accuracy_test:  0.391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yCcRWR6BD2A7",
        "colab_type": "code",
        "outputId": "bc108e2b-db1d-487c-f1fe-374ecbcb81ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Resultados obtidos:\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + MSELoss + ReLU(sem particionar o dataset de treino): \n",
        "###### Loss_train = 0.08425  \n",
        "###### Loss_valid = 0       ###### Accuracy_valid = 0\n",
        "###### Loss_test  = 0.08443 ###### Accuracy_test =  0.5806\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + MSELoss + ReLU(particionando o dataset de treino*): \n",
        "###### Loss_train = 0.0846 \n",
        "###### Loss_valid = 0.0847 ###### Accuracy_valid = 0.2844\n",
        "###### Loss_test  = 0.0849 ###### Accuracy_test  = 0.2868\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + SGD + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9290 \n",
        "###### Loss_valid = 1.9698 ###### Accuracy_valid = 0.336\n",
        "###### Loss_test  = 1.9513 ###### Accuracy_test  = 0.3353\n",
        "\n",
        "\n",
        "\n",
        "# ---> MLP com 1 camada(s)(20) oculta(s) de 20 perceptrons + Adam + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9606 \n",
        "###### Loss_valid = 2.0047 ###### Accuracy_valid = 0.3042\n",
        "###### Loss_test  = 2.0014 ###### Accuracy_test  = 0.3032\n",
        "\n",
        "\n",
        "# ---> MLP com 2 camada(s) oculta(s)(50 e 20) de 20 perceptrons + Adam + CrossEntropyLoss + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.7311 \n",
        "###### Loss_valid = 1.8388 ###### Accuracy_valid = 0.3738\n",
        "###### Loss_test  = 1.8464 ###### Accuracy_test  = 0.3585\n",
        "\n",
        "\n",
        "# ---> MLP com 2 camada(s) oculta(s)(50 e 20) de 20 perceptrons + SGD* + MSELoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 0.0936 \n",
        "###### Loss_valid = 0.0938 ###### Accuracy_valid = 0.18\n",
        "###### Loss_test  = 0.0934 ###### Accuracy_test  = 0.1816\n",
        "\n",
        "\n",
        "# ---> MLP com 4 camada(s) ocultas(100 80 50 e 20) de 20 perceptrons + SGD + MSELoss + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.9197 \n",
        "###### Loss_valid = 1.9108 ###### Accuracy_valid = 0.3156\n",
        "###### Loss_test  = 1.9309 ###### Accuracy_test  = 0.3026\n",
        "\n",
        "\n",
        "#****** ---> MLP com 4 camada(s) ocultas(100 80 50 e 20) de 20 perceptrons + Adam* + CrossEntropyLoss* + ReLU(particionando o dataset de treino):\n",
        "###### Loss_train = 1.5833 \n",
        "###### Loss_valid = 1.9346 ###### Accuracy_valid = 0.358\n",
        "###### Loss_test  = 1.9337 ###### Accuracy_test  = 0.3708\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''test\n",
        "'''\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'s\\nsfafs\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ImnggM95Gb8L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hiB6jvf0VIL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusões\n",
        "\n",
        "Depois de testar os diversos parâmetros da MLP é possível perceber a tamanha diversidade e opções para moldá-la de acordo com o domínio ou projeto, o que é muito bom comparado ao modo que faziamos no perceptron. No primeiro trabalho foi difícil e demorado de fazer cada neurônio separado e depois juntá-los, porém no final tinhamos a melhor resposta possível(tirando o fato de ajustar apenas o neta). \n",
        "\n",
        "Entretanto, na MLP, é possível alterar diversos aspectos, como a arquitetura, tipo de loss, tipo de optimizador, porém é mais fácil e rápido de implementar, graças ao pytorch. Por outro lado, se perde muito tempo achando os parâmetros certos para que a rede consiga atingir o máximo de acurácia possível(no caso esperar que o treinamento acabe). \n",
        "\n",
        "\n",
        "É preciso mencionar também que a acurácia baixou drasticamente com o particionamento dos exemplos de treinamento e avaliação. O que eu já sabia que ia acontecer, entretanto seja melhor botar mais exemplos no treinamento já que eu apenas separei alguns para a avaliação.\n",
        "\n",
        "E sobre o modelo final que escolhi como um dos melhores, foi o que possuia 4 camadas ocultas, sendo elas respectivamente de tamanhos: 100, 80, 50, 20. Eu optei em fazer uma escada como foi recomendado em aula, justamente pelo fato do aprendizado da rede neural ser como uma, onde se aprende o básico e depois irá subir até convergir para uma saída/porta. \n",
        "\n",
        "Eu comecei a usar o MSELoss mas depois de tentar todas as combinações cheguei a conclusão que o CrossEntropy é muito bom para a rede, pois observei que a Loss converge muito mais rápido do que o MSE. E também como vimos em aula o CrossEntropy é o mais recomendado para problemas de classificação.\n",
        "\n",
        "O otimizador que estou usando é o Adam, que foi muito bem combinado com o CrossEntropy.(Dados das redes célula a cima)\n",
        "\n",
        "Depois de definir qual loss e qual otimizador, testei várias arquiteturas. A partir da 3 camada oculta não dava tanto avanço nos resultados. Então optei por deixar ela pequena com 3 camadas de 80, 70 e 50, respectvamente. E os resultados foram os seguintes:\n",
        "\n",
        "\n",
        "#MLP(\n",
        "  (fc1): Linear(in_features=1024, out_features=80, bias=True)\n",
        "  (fc2): Linear(in_features=80, out_features=70, bias=True)\n",
        "  (fc3): Linear(in_features=70, out_features=50, bias=True)\n",
        "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
        "  (activation_function): ReLU()\n",
        "#)\n",
        "\n",
        "#-----\n",
        "\n",
        "\n",
        ">Média do Loss no treinamento: 1.4947\n",
        "\n",
        "\n",
        ">Média da Loss na Avaliação:     1.8225\n",
        "\n",
        "\n",
        ">Média da Loss no Teste:            1.8141\n",
        "\n",
        "\n",
        "#-----\n",
        "\n",
        "\n",
        ">Acurácia na Avaliação:               39,82%\n",
        "\n",
        ">Acurácia no Teste:                      39,1%\n",
        "\n"
      ]
    }
  ]
}